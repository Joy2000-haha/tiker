import pandas as pd
import numpy as np
import wrds
import warnings
import os
from datetime import datetime
import traceback

# å¿½ç•¥ä¸€äº›pandasçš„è­¦å‘Š
warnings.filterwarnings('ignore')


class StockDescriptiveStats:
    def __init__(self, ticker_csv_path):
        """
        åˆå§‹åŒ–è‚¡ç¥¨æè¿°æ€§ç»Ÿè®¡åˆ†æç±»
        """
        self.ticker_csv_path = ticker_csv_path
        self.db = None
        self.tickers = None
        # åˆå§‹åŒ–ç”¨äºå­˜å‚¨æ•°æ®çš„DataFrame
        self.crsp_data = None
        self.comp_data = None
        self.ccm_link = None
        self.merged_data = None
        self.final_stats = None
        self.results = None
        self.detailed_results = None
        self.summary_table = None
        self.percentile_table = None

    def connect_wrds(self):
        """è¿æ¥WRDSæ•°æ®åº“"""
        try:
            self.db = wrds.Connection()
            print("æˆåŠŸè¿æ¥åˆ°WRDSæ•°æ®åº“")
            return True
        except Exception as e:
            print(f"è¿æ¥WRDSæ•°æ®åº“å¤±è´¥: {e}")
            return False

    def load_tickers(self):
        """ä»CSVæ–‡ä»¶åŠ è½½tickeråˆ—è¡¨"""
        try:
            ticker_df = pd.read_csv(self.ticker_csv_path)
            # å‡è®¾CSVæ–‡ä»¶æœ‰ä¸€åˆ—åŒ…å«tickerç¬¦å·
            possible_columns = ['ticker', 'TICKER', 'symbol', 'SYMBOL', 'permno', 'PERMNO']
            ticker_column = None
            for col in possible_columns:
                if col in ticker_df.columns:
                    ticker_column = col
                    break
            if ticker_column is None:
                ticker_column = ticker_df.columns[0]
                print(f"æœªæ‰¾åˆ°æ ‡å‡†tickeråˆ—åï¼Œå°†ä½¿ç”¨ç¬¬ä¸€åˆ—: '{ticker_column}'")

            self.tickers = ticker_df[ticker_column].dropna().unique().tolist()
            print(f"æˆåŠŸåŠ è½½ {len(self.tickers)} ä¸ªè‚¡ç¥¨ä»£ç ")
            return True
        except Exception as e:
            print(f"åŠ è½½tickeråˆ—è¡¨å¤±è´¥: {e}")
            return False

    def get_ccm_link(self):
        """è·å–CRSP-Compustaté“¾æ¥è¡¨"""
        try:
            print("æ­£åœ¨è·å–CRSP-Compustaté“¾æ¥è¡¨...")
            ccm_query = """
                SELECT gvkey, lpermno as permno, linktype, linkprim, linkdt, linkenddt
                FROM crsp.ccmxpf_linktable
                WHERE linktype IN ('LU', 'LC') AND linkprim IN ('P', 'C')
            """
            self.ccm_link = self.db.raw_sql(ccm_query, date_cols=['linkdt', 'linkenddt'])

            # å¤„ç†é“¾æ¥ç»“æŸæ—¥æœŸï¼ˆå¦‚æœä¸ºç©ºï¼Œè®¾ä¸ºå½“å‰æ—¥æœŸï¼‰
            self.ccm_link['linkenddt'] = self.ccm_link['linkenddt'].fillna(pd.to_datetime('2024-12-31'))

            print(f"æˆåŠŸè·å–CCMé“¾æ¥è¡¨: {len(self.ccm_link)} æ¡è®°å½•")
            return True
        except Exception as e:
            print(f"è·å–CCMé“¾æ¥è¡¨å¤±è´¥: {e}")
            traceback.print_exc()
            return False

    def get_stock_data(self, start_date, end_date):
        """è·å–CRSPå’ŒCompustatæ•°æ®"""
        if not self.tickers:
            print("Tickeråˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•è·å–æ•°æ®ã€‚")
            return False

        # --- æ­¥éª¤ 1: è·å–CCMé“¾æ¥è¡¨ ---
        if not self.get_ccm_link():
            return False

        # --- æ­¥éª¤ 2: è·å–CRSPæœˆåº¦æ•°æ® ---
        print("æ­£åœ¨è·å–CRSPæ•°æ®...")
        try:
            # åˆ†å—å¤„ç†ä»¥é¿å…æŸ¥è¯¢è¿‡é•¿
            chunk_size = 500
            all_crsp_data = []

            for i in range(0, len(self.tickers), chunk_size):
                chunk = self.tickers[i:i + chunk_size]

                # å¤„ç†å•ä¸ªtickerçš„æƒ…å†µï¼ˆé¿å…SQLè¯­æ³•é”™è¯¯ï¼‰
                if len(chunk) == 1:
                    ticker_condition = f"= '{chunk[0]}'"
                else:
                    ticker_condition = f"IN {tuple(chunk)}"

                query = f"""
                    SELECT a.permno, a.permco, a.date, b.ticker, a.ret, a.retx, a.prc, a.shrout, a.vol
                    FROM crsp.msf as a
                    LEFT JOIN crsp.msenames as b
                    ON a.permno = b.permno AND b.namedt <= a.date AND a.date <= b.nameendt
                    WHERE a.date >= '{start_date}' AND a.date <= '{end_date}'
                    AND b.ticker {ticker_condition}
                    AND a.ret IS NOT NULL
                    ORDER BY a.permno, a.date
                """

                stock_chunk = self.db.raw_sql(query, date_cols=['date'])
                if not stock_chunk.empty:
                    all_crsp_data.append(stock_chunk)

            if not all_crsp_data:
                print("æœªè·å–åˆ°ä»»ä½•CRSPæ•°æ®ã€‚")
                return False

            self.crsp_data = pd.concat(all_crsp_data, ignore_index=True)
            self.crsp_data.drop_duplicates(subset=['permno', 'date'], keep='first', inplace=True)
            print(f"æˆåŠŸè·å–CRSPæ•°æ®: {len(self.crsp_data)} æ¡è®°å½•")

        except Exception as e:
            print(f"è·å–CRSPæ•°æ®å¤±è´¥: {e}")
            traceback.print_exc()
            return False

        # --- æ­¥éª¤ 3: è·å–Compustatå¹´åº¦æ•°æ® ---
        print("æ­£åœ¨è·å–Compustatæ•°æ®...")
        try:
            # ä¿®æ­£ï¼šä½¿ç”¨æ­£ç¡®çš„è¡¨å comp.funda
            comp_query = f"""
                SELECT gvkey, datadate, at, lt, pstk, ceq, txditc, pstkrv, seq, pstkl
                FROM comp.funda
                WHERE datadate >= '{start_date}' AND datadate <= '{end_date}'
                AND datafmt = 'STD' AND consol = 'C' AND indfmt = 'INDL'
                AND at IS NOT NULL
                ORDER BY gvkey, datadate
            """

            self.comp_data = self.db.raw_sql(comp_query, date_cols=['datadate'])
            print(f"æˆåŠŸè·å–Compustatæ•°æ®: {len(self.comp_data)} æ¡è®°å½•")

        except Exception as e:
            print(f"è·å–Compustatæ•°æ®å¤±è´¥: {e}")
            traceback.print_exc()
            return False

        print(f"æˆåŠŸè·å–æ‰€æœ‰æ•°æ®")
        return True

    def calculate_indicators(self):
        """åŸºäºè·å–çš„æ•°æ®è®¡ç®—æ‰€éœ€æŒ‡æ ‡"""
        if self.crsp_data is None or self.comp_data is None or self.ccm_link is None:
            print("åŸºç¡€æ•°æ®ç¼ºå¤±ï¼Œæ— æ³•è®¡ç®—æŒ‡æ ‡ã€‚")
            return False

        print("æ­£åœ¨è®¡ç®—æŒ‡æ ‡...")
        try:
            # --- æ­¥éª¤ 1: åˆå¹¶CRSPå’ŒCCMé“¾æ¥è¡¨ ---
            print("æ­£åœ¨åˆå¹¶CRSPå’ŒCCMé“¾æ¥è¡¨...")
            crsp_ccm = self.crsp_data.merge(self.ccm_link, on='permno', how='left')

            # è¿‡æ»¤æœ‰æ•ˆçš„é“¾æ¥å…³ç³»
            crsp_ccm = crsp_ccm[
                (crsp_ccm['date'] >= crsp_ccm['linkdt']) &
                (crsp_ccm['date'] <= crsp_ccm['linkenddt']) &
                (crsp_ccm['gvkey'].notna())
                ]

            if crsp_ccm.empty:
                print("CRSPå’ŒCCMé“¾æ¥åæ²¡æœ‰æœ‰æ•ˆæ•°æ®")
                return False

            print(f"CRSP-CCMåˆå¹¶å: {len(crsp_ccm)} æ¡è®°å½•")

            # --- æ­¥éª¤ 2: å¤„ç†Compustatæ•°æ® ---
            print("æ­£åœ¨å¤„ç†Compustatæ•°æ®...")
            comp = self.comp_data.copy()

            # è®¡ç®—è´¦é¢ä»·å€¼ (Book Equity) - å‚è€ƒKen French's libraryå®šä¹‰
            comp['txditc'] = comp['txditc'].fillna(0)
            comp['ps'] = comp['pstkrv'].fillna(comp['pstkl']).fillna(comp['pstk']).fillna(0)
            comp['se'] = comp['seq'].fillna(comp['ceq'] + comp['pstk'].fillna(0)).fillna(
                comp['at'] - comp['lt']).fillna(0)
            comp['be'] = comp['se'] + comp['txditc'] - comp['ps']

            # åªä¿ç•™è´¦é¢ä»·å€¼ä¸ºæ­£çš„è®°å½•
            comp = comp[comp['be'] > 0]

            # è´¢åŠ¡æ•°æ®é€šå¸¸å»¶è¿Ÿ6ä¸ªæœˆå¯ç”¨
            comp['available_date'] = comp['datadate'] + pd.DateOffset(months=6)

            print(f"å¤„ç†åçš„Compustatæ•°æ®: {len(comp)} æ¡è®°å½•")

            # --- æ­¥éª¤ 3: åˆå¹¶æ‰€æœ‰æ•°æ® ---
            print("æ­£åœ¨åˆå¹¶CRSPå’ŒCompustatæ•°æ®...")

            # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            crsp_ccm['date'] = pd.to_datetime(crsp_ccm['date'])
            comp['available_date'] = pd.to_datetime(comp['available_date'])

            # ç¡®ä¿gvkeyæ•°æ®ç±»å‹ä¸€è‡´
            crsp_ccm['gvkey'] = crsp_ccm['gvkey'].astype(str)
            comp['gvkey'] = comp['gvkey'].astype(str)

            # æ’åºæ•°æ®ä»¥ä¾¿ä½¿ç”¨merge_asof - é‡è¦ï¼šå¿…é¡»å…ˆæŒ‰byé”®æ’åºï¼Œå†æŒ‰æ—¶é—´é”®æ’åº
            print("æ­£åœ¨æ’åºæ•°æ®...")
            crsp_ccm = crsp_ccm.sort_values(['gvkey', 'date']).reset_index(drop=True)
            comp = comp.sort_values(['gvkey', 'available_date']).reset_index(drop=True)

            # æ£€æŸ¥æ’åºæ˜¯å¦æ­£ç¡®
            print("éªŒè¯æ•°æ®æ’åº...")
            print(f"CRSPæ•°æ®æ ·æœ¬ - gvkey: {crsp_ccm['gvkey'].head()}")
            print(f"CRSPæ•°æ®æ ·æœ¬ - date: {crsp_ccm['date'].head()}")
            print(f"Compæ•°æ®æ ·æœ¬ - gvkey: {comp['gvkey'].head()}")
            print(f"Compæ•°æ®æ ·æœ¬ - available_date: {comp['available_date'].head()}")

            print(f"CRSPæ•°æ®ç±»å‹ - gvkey: {crsp_ccm['gvkey'].dtype}, date: {crsp_ccm['date'].dtype}")
            print(f"Compæ•°æ®ç±»å‹ - gvkey: {comp['gvkey'].dtype}, available_date: {comp['available_date'].dtype}")
            print(f"CRSPç©ºå€¼ - gvkey: {crsp_ccm['gvkey'].isna().sum()}, date: {crsp_ccm['date'].isna().sum()}")
            print(
                f"Compç©ºå€¼ - gvkey: {comp['gvkey'].isna().sum()}, available_date: {comp['available_date'].isna().sum()}")

            # ç§»é™¤ç©ºå€¼
            crsp_ccm = crsp_ccm.dropna(subset=['gvkey', 'date'])
            comp = comp.dropna(subset=['gvkey', 'available_date'])

            print(f"æ¸…ç†å - CRSP: {len(crsp_ccm)} æ¡è®°å½•, Comp: {len(comp)} æ¡è®°å½•")

            # æ‰§è¡Œæ—¶é—´åºåˆ—åˆå¹¶...
            print("æ‰§è¡Œæ—¶é—´åºåˆ—åˆå¹¶...")

            # ä½¿ç”¨ä¼˜åŒ–çš„å‘é‡åŒ–åŒ¹é…æ–¹æ¡ˆ
            print("ä½¿ç”¨ä¼˜åŒ–çš„å‘é‡åŒ–æ—¶é—´åŒ¹é…æ–¹æ¡ˆ...")

            # æ¸…ç†å’Œé¢„å¤„ç†æ•°æ®
            print("æ¸…ç†å’Œé¢„å¤„ç†æ•°æ®...")

            # å»é™¤é‡å¤è®°å½•
            crsp_ccm = crsp_ccm.drop_duplicates(subset=['gvkey', 'date']).reset_index(drop=True)
            comp = comp.drop_duplicates(subset=['gvkey', 'available_date']).reset_index(drop=True)

            print(f"å»é‡å - CRSP: {len(crsp_ccm)} æ¡è®°å½•, Comp: {len(comp)} æ¡è®°å½•")

            # æ–¹æ³•1: å°è¯•ä½¿ç”¨pandasçš„merge_asofä½†å¤„ç†æ’åºé—®é¢˜
            try:
                print("å°è¯•ä¿®å¤æ’åºåçš„merge_asof...")

                # æ›´ä¸¥æ ¼çš„æ•°æ®æ¸…ç†
                crsp_clean = crsp_ccm.copy()
                comp_clean = comp.copy()

                # ç¡®ä¿æ²¡æœ‰é‡å¤å’ŒNaN
                crsp_clean = crsp_clean.dropna(subset=['gvkey', 'date'])
                comp_clean = comp_clean.dropna(subset=['gvkey', 'available_date'])

                # è½¬æ¢æ•°æ®ç±»å‹ç¡®ä¿ä¸€è‡´æ€§
                crsp_clean['gvkey'] = crsp_clean['gvkey'].astype(str).str.strip()
                comp_clean['gvkey'] = comp_clean['gvkey'].astype(str).str.strip()

                # å¼ºåˆ¶æ’åº
                crsp_clean = crsp_clean.sort_values(['gvkey', 'date']).reset_index(drop=True)
                comp_clean = comp_clean.sort_values(['gvkey', 'available_date']).reset_index(drop=True)

                # éªŒè¯æ’åº
                if (crsp_clean.groupby('gvkey')['date'].is_monotonic_increasing.all() and
                        comp_clean.groupby('gvkey')['available_date'].is_monotonic_increasing.all()):

                    merged = pd.merge_asof(
                        crsp_clean,
                        comp_clean[['gvkey', 'available_date', 'be']],
                        left_on='date',
                        right_on='available_date',
                        by='gvkey',
                        direction='backward',
                        tolerance=pd.Timedelta(days=365 * 5)
                    )
                    print("ä¿®å¤åçš„merge_asofæˆåŠŸï¼")
                else:
                    raise ValueError("æ’åºéªŒè¯å¤±è´¥")

            except Exception as e:
                print(f"merge_asofä»ç„¶å¤±è´¥: {e}")
                print("ä½¿ç”¨å¿«é€Ÿå‘é‡åŒ–åŒ¹é…æ–¹æ¡ˆ...")

                # æ–¹æ³•2: ä½¿ç”¨æ›´å¿«çš„å‘é‡åŒ–æ–¹æ³•
                try:
                    # ä¸ºæ¯ä¸ªgvkeyåˆ›å»ºåŒ¹é…ç´¢å¼•
                    merged_list = []

                    # æŒ‰gvkeyåˆ†ç»„å¤„ç† - å‘é‡åŒ–æ–¹æ³•
                    for gvkey in crsp_ccm['gvkey'].unique():
                        if pd.isna(gvkey):
                            continue

                        crsp_subset = crsp_ccm[crsp_ccm['gvkey'] == gvkey].copy()
                        comp_subset = comp[comp['gvkey'] == gvkey].copy()

                        if comp_subset.empty:
                            continue

                        # æ’åº
                        crsp_subset = crsp_subset.sort_values('date').reset_index(drop=True)
                        comp_subset = comp_subset.sort_values('available_date').reset_index(drop=True)

                        # ä½¿ç”¨numpyçš„searchsortedè¿›è¡Œå¿«é€ŸåŒ¹é…
                        crsp_dates = crsp_subset['date'].values
                        comp_dates = comp_subset['available_date'].values

                        # æ‰¾åˆ°æ¯ä¸ªCRSPæ—¥æœŸå¯¹åº”çš„æœ€æ–°Compæ—¥æœŸç´¢å¼•
                        indices = np.searchsorted(comp_dates, crsp_dates, side='right') - 1

                        # åªä¿ç•™æœ‰æ•ˆåŒ¹é…ï¼ˆindices >= 0ï¼‰
                        valid_mask = indices >= 0
                        valid_crsp = crsp_subset[valid_mask].copy()
                        valid_indices = indices[valid_mask]

                        if len(valid_crsp) > 0:
                            # æ·»åŠ åŒ¹é…çš„beå€¼
                            valid_crsp['be'] = comp_subset.iloc[valid_indices]['be'].values
                            valid_crsp['available_date'] = comp_subset.iloc[valid_indices]['available_date'].values
                            merged_list.append(valid_crsp)

                    if merged_list:
                        merged = pd.concat(merged_list, ignore_index=True)
                        print(f"å¿«é€Ÿå‘é‡åŒ–åŒ¹é…æˆåŠŸï¼åˆå¹¶æ•°æ®: {len(merged)} æ¡è®°å½•")
                    else:
                        raise ValueError("å‘é‡åŒ–åŒ¹é…å¤±è´¥")

                except Exception as e2:
                    print(f"å‘é‡åŒ–åŒ¹é…ä¹Ÿå¤±è´¥: {e2}")
                    print("ä½¿ç”¨æœ€ç®€å•çš„å¹´åº¦åŒ¹é…ä½œä¸ºæœ€åå¤‡é€‰...")

                    # æ–¹æ³•3: ç®€å•å¹´åº¦åŒ¹é…
                    crsp_ccm['year'] = crsp_ccm['date'].dt.year
                    comp['year'] = comp['datadate'].dt.year

                    # ä¸ºæ¯ä¸ªgvkey-yearç»„åˆé€‰æ‹©æœ€æ–°çš„è´¢åŠ¡æ•°æ®
                    comp_yearly = comp.sort_values(['gvkey', 'year', 'datadate']).groupby(
                        ['gvkey', 'year']).last().reset_index()

                    merged = crsp_ccm.merge(
                        comp_yearly[['gvkey', 'year', 'be']],
                        on=['gvkey', 'year'],
                        how='left'
                    )

                    # è¿‡æ»¤æ‰æ²¡æœ‰è´¢åŠ¡æ•°æ®çš„è®°å½•
                    merged = merged[merged['be'].notna()]

                    if merged.empty:
                        print("æ‰€æœ‰åˆå¹¶æ–¹æ¡ˆéƒ½å¤±è´¥")
                        return False
                    else:
                        print(f"å¹´åº¦åŒ¹é…æˆåŠŸï¼åˆå¹¶æ•°æ®: {len(merged)} æ¡è®°å½•")

            # åªä¿ç•™æœ‰è´¦é¢ä»·å€¼æ•°æ®çš„è®°å½•
            merged = merged[merged['be'].notna()]

            if merged.empty:
                print("åˆå¹¶åæ²¡æœ‰æœ‰æ•ˆæ•°æ®")
                return False

            print(f"æœ€ç»ˆåˆå¹¶æ•°æ®: {len(merged)} æ¡è®°å½•")

            # --- æ­¥éª¤ 4: è®¡ç®—å…³é”®æŒ‡æ ‡ ---
            print("æ­£åœ¨è®¡ç®—å…³é”®æŒ‡æ ‡...")

            df = merged.copy()

            # è®¡ç®—å¸‚å€¼ (Market Equity)
            df['prc_abs'] = df['prc'].abs()
            df['me'] = df['prc_abs'] * df['shrout']  # å•ä½: åƒç¾å…ƒ

            # è®¡ç®—è´¦é¢å¸‚å€¼æ¯” (B/M)
            df['bm'] = (df['be'] * 1000) / df['me']  # å°†è´¦é¢ä»·å€¼ä»ç™¾ä¸‡è½¬ä¸ºåƒç¾å…ƒ

            # è®¡ç®—size (å–å¯¹æ•°)
            df['size'] = np.log(df['me'])

            # è®¡ç®—åŠ¨é‡æŒ‡æ ‡ (mom) - è¿‡å»12ä¸ªæœˆæ”¶ç›Šç‡ï¼Œè·³è¿‡æœ€è¿‘1ä¸ªæœˆ
            df = df.sort_values(['permno', 'date'])
            df['ret_lag1'] = df.groupby('permno')['ret'].shift(1)
            df['ret_cumulative'] = df.groupby('permno')['ret'].rolling(window=12, min_periods=8).apply(
                lambda x: (1 + x).prod() - 1, raw=False
            ).reset_index(0, drop=True)
            df['mom'] = df['ret_cumulative'] - df['ret_lag1']

            # è®¡ç®—æ¯æœˆçš„æˆªé¢æ•°æ®ä¸ªæ•° (n)
            df['year'] = df['date'].dt.year
            df['month'] = df['date'].dt.month
            df['n'] = df.groupby(['year', 'month'])['permno'].transform('count')

            # é€‰å–æœ€ç»ˆéœ€è¦çš„åˆ—å¹¶å»é™¤ç¼ºå¤±å€¼
            final_columns = ['permno', 'ticker', 'date', 'year', 'month', 'ret', 'size', 'bm', 'mom', 'n']
            self.final_stats = df[final_columns].dropna(subset=['ret', 'size', 'bm'])

            print(f"æŒ‡æ ‡è®¡ç®—å®Œæˆï¼Œæœ€ç»ˆæ•°æ®: {len(self.final_stats)} æ¡è®°å½•")
            print(f"æ—¶é—´èŒƒå›´: {self.final_stats['date'].min()} åˆ° {self.final_stats['date'].max()}")
            print(f"è‚¡ç¥¨æ•°é‡: {self.final_stats['permno'].nunique()}")

            return True

        except Exception as e:
            print(f"è®¡ç®—æŒ‡æ ‡å¤±è´¥: {e}")
            traceback.print_exc()
            return False

    def calculate_descriptive_stats(self):
        """è®¡ç®—æè¿°æ€§ç»Ÿè®¡ - ä¸ªè‚¡cross-sectionalå†time series average+std"""
        if self.final_stats is None or self.final_stats.empty:
            print("æ²¡æœ‰å¯ç”¨äºç»Ÿè®¡çš„æŒ‡æ ‡æ•°æ®ã€‚")
            return False

        print("æ­£åœ¨è®¡ç®—æè¿°æ€§ç»Ÿè®¡...")
        print("æ–¹æ³•ï¼šä¸ªè‚¡cross-sectionalå†time series average+std")
        try:
            # å®šä¹‰è¦åˆ†æçš„å˜é‡
            variables = ['ret', 'size', 'bm', 'mom', 'n']
            # æ‰©å±•ç™¾åˆ†ä½æ•°åˆ—è¡¨ - ä»1%åˆ°99%
            percentiles = [0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99]
            # æ·»åŠ æ›´å¤šå…³é”®ç™¾åˆ†ä½æ•°
            extended_percentiles = [0.01, 0.02, 0.03, 0.04, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50,
                                    0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 0.96, 0.97, 0.98, 0.99]

            results = {}
            detailed_results = {}  # å­˜å‚¨è¯¦ç»†çš„æœˆåº¦ç»Ÿè®¡

            for var in variables:
                print(f"æ­£åœ¨è®¡ç®— {var} çš„æè¿°æ€§ç»Ÿè®¡...")

                if var == 'n':
                    # nçš„ç‰¹æ®Šå¤„ç†ï¼ˆæ¯æœˆç›¸åŒï¼‰
                    n_monthly = self.final_stats.groupby(['year', 'month'])[var].first()
                    results[var] = {
                        'mean': n_monthly.mean(),
                        'std': n_monthly.std(),
                        'min': n_monthly.min(),
                        'max': n_monthly.max(),
                        'count': len(n_monthly),
                        **{f'p{int(p * 100)}': np.percentile(n_monthly, p * 100) for p in extended_percentiles}
                    }
                    detailed_results[var] = {
                        'monthly_values': n_monthly,
                        'time_series_stats': {
                            'mean': n_monthly.mean(),
                            'std': n_monthly.std()
                        }
                    }
                else:
                    # æ­¥éª¤1: è®¡ç®—æ¯æœˆçš„æˆªé¢ç»Ÿè®¡ (Cross-sectional statistics for each time period)
                    print(f"  - è®¡ç®— {var} çš„æœˆåº¦æˆªé¢ç»Ÿè®¡...")

                    def calculate_cross_sectional_stats(group):
                        """è®¡ç®—å•æœˆçš„æˆªé¢ç»Ÿè®¡"""
                        data = group.dropna()
                        if len(data) == 0:
                            return pd.Series({
                                'count': 0,
                                'mean': np.nan,
                                'std': np.nan,
                                'min': np.nan,
                                'max': np.nan,
                                'skew': np.nan,
                                'kurt': np.nan,
                                **{f'p{int(p * 100)}': np.nan for p in extended_percentiles}
                            })

                        # åŸºç¡€ç»Ÿè®¡é‡
                        stats_dict = {
                            'count': len(data),
                            'mean': data.mean(),
                            'std': data.std(),
                            'min': data.min(),
                            'max': data.max(),
                            'skew': data.skew() if len(data) >= 3 else np.nan,
                            'kurt': data.kurtosis() if len(data) >= 4 else np.nan,
                        }

                        # ç™¾åˆ†ä½æ•°
                        for p in extended_percentiles:
                            stats_dict[f'p{int(p * 100)}'] = np.percentile(data, p * 100)

                        return pd.Series(stats_dict)

                    # æŒ‰å¹´æœˆåˆ†ç»„è®¡ç®—æˆªé¢ç»Ÿè®¡
                    monthly_cross_sectional = self.final_stats.groupby(['year', 'month'])[var].apply(
                        calculate_cross_sectional_stats
                    ).unstack(fill_value=np.nan)

                    # æ·»åŠ æ—¥æœŸç´¢å¼•
                    monthly_cross_sectional.index = pd.to_datetime(
                        monthly_cross_sectional.index.get_level_values(0).astype(str) + '-' +
                        monthly_cross_sectional.index.get_level_values(1).astype(str) + '-01'
                    )

                    print(f"  - {var} æœˆåº¦æˆªé¢ç»Ÿè®¡å®Œæˆï¼Œæ—¶é—´åºåˆ—é•¿åº¦: {len(monthly_cross_sectional)}")

                    # æ­¥éª¤2: è®¡ç®—æˆªé¢ç»Ÿè®¡çš„æ—¶é—´åºåˆ—ç»Ÿè®¡ (Time series statistics of cross-sectional statistics)
                    print(f"  - è®¡ç®— {var} çš„æ—¶é—´åºåˆ—ç»Ÿè®¡...")

                    def safe_ts_stats(series):
                        """å®‰å…¨è®¡ç®—æ—¶é—´åºåˆ—ç»Ÿè®¡"""
                        clean_series = series.dropna()
                        if len(clean_series) == 0:
                            return {'mean': np.nan, 'std': np.nan, 'min': np.nan, 'max': np.nan, 'count': 0}
                        return {
                            'mean': clean_series.mean(),
                            'std': clean_series.std(),
                            'min': clean_series.min(),
                            'max': clean_series.max(),
                            'count': len(clean_series)
                        }

                    # å¯¹æ¯ä¸ªæˆªé¢ç»Ÿè®¡é‡è®¡ç®—æ—¶é—´åºåˆ—ç»Ÿè®¡
                    ts_stats = {}
                    for col in monthly_cross_sectional.columns:
                        ts_stats[f'ts_{col}'] = safe_ts_stats(monthly_cross_sectional[col])

                    # æ­¥éª¤3: è®¡ç®—æ€»ä½“ç»Ÿè®¡ (Overall statistics across all observations)
                    print(f"  - è®¡ç®— {var} çš„æ€»ä½“ç»Ÿè®¡...")
                    overall_data = self.final_stats[var].dropna()

                    overall_stats = {
                        'overall_count': len(overall_data),
                        'overall_mean': overall_data.mean(),
                        'overall_std': overall_data.std(),
                        'overall_min': overall_data.min(),
                        'overall_max': overall_data.max(),
                        'overall_skew': overall_data.skew() if len(overall_data) >= 3 else np.nan,
                        'overall_kurt': overall_data.kurtosis() if len(overall_data) >= 4 else np.nan,
                    }

                    # æ€»ä½“ç™¾åˆ†ä½æ•°
                    for p in extended_percentiles:
                        overall_stats[f'overall_p{int(p * 100)}'] = np.percentile(overall_data, p * 100)

                    # åˆå¹¶æ‰€æœ‰ç»Ÿè®¡ç»“æœ
                    results[var] = {**ts_stats, **overall_stats}
                    detailed_results[var] = {
                        'monthly_cross_sectional': monthly_cross_sectional,
                        'time_series_stats': ts_stats,
                        'overall_stats': overall_stats
                    }

                    print(f"  - {var} ç»Ÿè®¡è®¡ç®—å®Œæˆ")

            self.results = results
            self.detailed_results = detailed_results

            # ç”Ÿæˆæ±‡æ€»è¡¨
            print("æ­£åœ¨ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡è¡¨...")
            self._generate_comprehensive_summary_table(variables, extended_percentiles)

            print("æˆåŠŸç”Ÿæˆæ±‡æ€»è¡¨ã€‚")
            return True

        except Exception as e:
            print(f"è®¡ç®—æè¿°æ€§ç»Ÿè®¡å¤±è´¥: {e}")
            traceback.print_exc()
            return False

    def _generate_comprehensive_summary_table(self, variables, percentiles):
        """ç”Ÿæˆç»¼åˆæ±‡æ€»ç»Ÿè®¡è¡¨"""
        try:
            summary_data = []

            for var in variables:
                stats = self.results[var]

                if var == 'n':
                    row = {
                        'Variable': var,
                        'Description': 'æ¯æœˆæˆªé¢æ•°æ®ä¸ªæ•°',
                        'Time_Series_Length': stats['count'],
                        'Mean': f"{stats['mean']:.2f}",
                        'Std': f"{stats['std']:.2f}",
                        'Min': f"{stats['min']:.2f}",
                        'Max': f"{stats['max']:.2f}",
                        'P1': f"{stats['p1']:.2f}",
                        'P5': f"{stats['p5']:.2f}",
                        'P10': f"{stats['p10']:.2f}",
                        'P25': f"{stats['p25']:.2f}",
                        'P50': f"{stats['p50']:.2f}",
                        'P75': f"{stats['p75']:.2f}",
                        'P90': f"{stats['p90']:.2f}",
                        'P95': f"{stats['p95']:.2f}",
                        'P99': f"{stats['p99']:.2f}",
                        'CS_Mean_TS_Mean': 'N/A',
                        'CS_Mean_TS_Std': 'N/A',
                        'CS_Std_TS_Mean': 'N/A',
                        'CS_Std_TS_Std': 'N/A',
                        'CS_Skew_TS_Mean': 'N/A',
                        'CS_Kurt_TS_Mean': 'N/A'
                    }
                else:
                    # è·å–å…³é”®çš„æ—¶é—´åºåˆ—ç»Ÿè®¡
                    cs_mean_ts = stats.get('ts_mean', {})
                    cs_std_ts = stats.get('ts_std', {})
                    cs_skew_ts = stats.get('ts_skew', {})
                    cs_kurt_ts = stats.get('ts_kurt', {})

                    row = {
                        'Variable': var,
                        'Description': self._get_variable_description(var),
                        'Time_Series_Length': cs_mean_ts.get('count', 0),
                        'Overall_Mean': f"{stats['overall_mean']:.6f}",
                        'Overall_Std': f"{stats['overall_std']:.6f}",
                        'Overall_Min': f"{stats['overall_min']:.6f}",
                        'Overall_Max': f"{stats['overall_max']:.6f}",
                        'Overall_Skew': f"{stats['overall_skew']:.4f}",
                        'Overall_Kurt': f"{stats['overall_kurt']:.4f}",
                        'Overall_Count': f"{stats['overall_count']:,}",

                        # æˆªé¢å‡å€¼çš„æ—¶é—´åºåˆ—ç»Ÿè®¡
                        'CS_Mean_TS_Mean': f"{cs_mean_ts.get('mean', np.nan):.6f}",
                        'CS_Mean_TS_Std': f"{cs_mean_ts.get('std', np.nan):.6f}",
                        'CS_Mean_TS_Min': f"{cs_mean_ts.get('min', np.nan):.6f}",
                        'CS_Mean_TS_Max': f"{cs_mean_ts.get('max', np.nan):.6f}",

                        # æˆªé¢æ ‡å‡†å·®çš„æ—¶é—´åºåˆ—ç»Ÿè®¡
                        'CS_Std_TS_Mean': f"{cs_std_ts.get('mean', np.nan):.6f}",
                        'CS_Std_TS_Std': f"{cs_std_ts.get('std', np.nan):.6f}",
                        'CS_Std_TS_Min': f"{cs_std_ts.get('min', np.nan):.6f}",
                        'CS_Std_TS_Max': f"{cs_std_ts.get('max', np.nan):.6f}",

                        # æˆªé¢ååº¦å’Œå³°åº¦çš„æ—¶é—´åºåˆ—å‡å€¼
                        'CS_Skew_TS_Mean': f"{cs_skew_ts.get('mean', np.nan):.4f}",
                        'CS_Kurt_TS_Mean': f"{cs_kurt_ts.get('mean', np.nan):.4f}",
                    }

                    # æ·»åŠ æ€»ä½“ç™¾åˆ†ä½æ•°
                    for p in [1, 5, 10, 25, 50, 75, 90, 95, 99]:
                        row[f'Overall_P{p}'] = f"{stats[f'overall_p{p}']:.6f}"

                summary_data.append(row)

            self.summary_table = pd.DataFrame(summary_data)

            # ç”Ÿæˆè¯¦ç»†çš„ç™¾åˆ†ä½æ•°åˆ†å¸ƒè¡¨
            self._generate_percentile_distribution_table(variables, percentiles)

        except Exception as e:
            print(f"ç”Ÿæˆæ±‡æ€»è¡¨å¤±è´¥: {e}")
            traceback.print_exc()

    def _generate_percentile_distribution_table(self, variables, percentiles):
        """ç”Ÿæˆè¯¦ç»†çš„ç™¾åˆ†ä½æ•°åˆ†å¸ƒè¡¨"""
        try:
            percentile_data = []

            for var in variables:
                if var == 'n':
                    continue

                stats = self.results[var]

                # æ¯ä¸ªç™¾åˆ†ä½æ•°ä¸€è¡Œ
                for p in percentiles:
                    p_int = int(p * 100)
                    percentile_data.append({
                        'Variable': var,
                        'Percentile': f'P{p_int}',
                        'Percentile_Value': p,
                        'Overall_Value': stats[f'overall_p{p_int}'],
                        'Description': f'{var}çš„ç¬¬{p_int}ç™¾åˆ†ä½æ•°'
                    })

            self.percentile_table = pd.DataFrame(percentile_data)

        except Exception as e:
            print(f"ç”Ÿæˆç™¾åˆ†ä½æ•°è¡¨å¤±è´¥: {e}")
            traceback.print_exc()

    def _get_variable_description(self, var):
        """è·å–å˜é‡æè¿°"""
        descriptions = {
            'ret': 'æœˆåº¦è‚¡ç¥¨æ”¶ç›Šç‡',
            'size': 'å¸‚å€¼å¯¹æ•°',
            'bm': 'è´¦é¢å¸‚å€¼æ¯”',
            'mom': 'åŠ¨é‡æŒ‡æ ‡(è¿‡å»11ä¸ªæœˆæ”¶ç›Šç‡)'
        }
        return descriptions.get(var, var)

    def save_results(self, output_dir='/Users/jinanwuyanzu/Desktop/News 5'):
        """ä¿å­˜ç»“æœåˆ°CSVæ–‡ä»¶"""
        if self.summary_table is None:
            print("æ²¡æœ‰ç»“æœå¯ä»¥ä¿å­˜ã€‚")
            return False

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)

        # å®šä¹‰æ–‡ä»¶è·¯å¾„
        summary_path = os.path.join(output_dir, 'stock_descriptive_summary.csv')
        percentile_path = os.path.join(output_dir, 'stock_percentile_distribution.csv')
        sample_path = os.path.join(output_dir, 'stock_sample_data.csv')
        monthly_stats_path = os.path.join(output_dir, 'stock_monthly_cross_sectional_stats.csv')

        print(f"æ­£åœ¨ä¿å­˜ç»“æœåˆ° {output_dir}...")
        try:
            # 1. ä¿å­˜ä¸»è¦æ±‡æ€»ç»Ÿè®¡è¡¨
            self.summary_table.to_csv(summary_path, index=False, encoding='utf-8')
            print(f"âœ“ ä¸»è¦æ±‡æ€»ç»Ÿè®¡å·²ä¿å­˜åˆ°: {summary_path}")

            # 2. ä¿å­˜ç™¾åˆ†ä½æ•°åˆ†å¸ƒè¡¨
            if hasattr(self, 'percentile_table') and self.percentile_table is not None:
                self.percentile_table.to_csv(percentile_path, index=False, encoding='utf-8')
                print(f"âœ“ ç™¾åˆ†ä½æ•°åˆ†å¸ƒå·²ä¿å­˜åˆ°: {percentile_path}")

            # 3. ä¿å­˜åŸå§‹æ•°æ®æ ·æœ¬
            if self.final_stats is not None and not self.final_stats.empty:
                sample_size = min(1000, len(self.final_stats))
                sample_data = self.final_stats.sample(n=sample_size, random_state=42)
                sample_data.to_csv(sample_path, index=False, encoding='utf-8')
                print(f"âœ“ æ ·æœ¬æ•°æ®å·²ä¿å­˜åˆ°: {sample_path}")

            # 4. ä¿å­˜è¯¦ç»†çš„æœˆåº¦æˆªé¢ç»Ÿè®¡ï¼ˆåˆå¹¶æ‰€æœ‰å˜é‡ï¼‰
            if hasattr(self, 'detailed_results'):
                monthly_stats_combined = []

                for var in ['ret', 'size', 'bm', 'mom']:
                    if var in self.detailed_results and 'monthly_cross_sectional' in self.detailed_results[var]:
                        monthly_df = self.detailed_results[var]['monthly_cross_sectional'].copy()
                        monthly_df['variable'] = var
                        monthly_df['date'] = monthly_df.index
                        monthly_stats_combined.append(monthly_df)

                if monthly_stats_combined:
                    combined_monthly = pd.concat(monthly_stats_combined, ignore_index=True)
                    # é‡æ–°æ’åˆ—åˆ—çš„é¡ºåº
                    cols = ['variable', 'date'] + [col for col in combined_monthly.columns if
                                                   col not in ['variable', 'date']]
                    combined_monthly = combined_monthly[cols]
                    combined_monthly.to_csv(monthly_stats_path, index=False, encoding='utf-8')
                    print(f"âœ“ æœˆåº¦æˆªé¢ç»Ÿè®¡å·²ä¿å­˜åˆ°: {monthly_stats_path}")

            # 5. ä¿å­˜ç»Ÿè®¡ç»“æœæ‘˜è¦æŠ¥å‘Š
            self._save_summary_report(output_dir)

            return True
        except Exception as e:
            print(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
            traceback.print_exc()
            return False

    def _save_summary_report(self, output_dir):
        """ä¿å­˜ç»Ÿè®¡ç»“æœæ‘˜è¦æŠ¥å‘Š"""
        try:
            report_path = os.path.join(output_dir, 'analysis_summary_report.txt')

            with open(report_path, 'w', encoding='utf-8') as f:
                f.write("=" * 80 + "\n")
                f.write("è‚¡ç¥¨æè¿°æ€§ç»Ÿè®¡åˆ†ææŠ¥å‘Š\n")
                f.write("=" * 80 + "\n\n")

                # æ•°æ®æ¦‚è§ˆ
                f.write("1. æ•°æ®æ¦‚è§ˆ\n")
                f.write("-" * 40 + "\n")
                if self.final_stats is not None:
                    f.write(
                        f"åˆ†ææ—¶é—´èŒƒå›´: {self.final_stats['date'].min().strftime('%Y-%m')} åˆ° {self.final_stats['date'].max().strftime('%Y-%m')}\n")
                    f.write(f"æ€»è§‚æµ‹æ•°: {len(self.final_stats):,}\n")
                    f.write(f"è‚¡ç¥¨æ•°é‡: {self.final_stats['permno'].nunique():,}\n")
                    f.write(f"å¹³å‡æ¯æœˆè‚¡ç¥¨æ•°: {self.final_stats.groupby(['year', 'month']).size().mean():.0f}\n")
                f.write("\n")

                # åˆ†ææ–¹æ³•
                f.write("2. åˆ†ææ–¹æ³•\n")
                f.write("-" * 40 + "\n")
                f.write("é‡‡ç”¨Cross-sectionalå†Time seriesç»Ÿè®¡æ–¹æ³•:\n")
                f.write("- æ­¥éª¤1: è®¡ç®—æ¯æœˆçš„æˆªé¢ç»Ÿè®¡é‡(å¹³å‡å€¼ã€æ ‡å‡†å·®ã€ç™¾åˆ†ä½æ•°ç­‰)\n")
                f.write("- æ­¥éª¤2: å¯¹æˆªé¢ç»Ÿè®¡é‡çš„æ—¶é—´åºåˆ—è®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®\n")
                f.write("- æ­¥éª¤3: æŠ¥å‘Šæ€»ä½“åˆ†å¸ƒçš„è¯¦ç»†ç™¾åˆ†ä½æ•°(1%-99%)\n")
                f.write("\n")

                # æ ¸å¿ƒå‘ç°
                f.write("3. æ ¸å¿ƒå‘ç°æ‘˜è¦\n")
                f.write("-" * 40 + "\n")

                if hasattr(self, 'results'):
                    for var in ['ret', 'size', 'bm', 'mom']:
                        if var in self.results:
                            stats = self.results[var]
                            f.write(f"\n{var.upper()} ({self._get_variable_description(var)}):\n")
                            f.write(f"  æ€»ä½“å‡å€¼: {stats['overall_mean']:.6f}\n")
                            f.write(f"  æ€»ä½“æ ‡å‡†å·®: {stats['overall_std']:.6f}\n")

                            ts_mean = stats.get('ts_mean', {})
                            ts_std = stats.get('ts_std', {})

                            f.write(f"  æˆªé¢å‡å€¼çš„æ—¶é—´åºåˆ—å‡å€¼: {ts_mean.get('mean', 'N/A')}\n")
                            f.write(f"  æˆªé¢å‡å€¼çš„æ—¶é—´åºåˆ—æ ‡å‡†å·®: {ts_mean.get('std', 'N/A')}\n")
                            f.write(f"  æˆªé¢æ ‡å‡†å·®çš„æ—¶é—´åºåˆ—å‡å€¼: {ts_std.get('mean', 'N/A')}\n")
                            f.write(f"  ä¸­ä½æ•°(P50): {stats['overall_p50']:.6f}\n")
                            f.write(f"  æç«¯å€¼: P1={stats['overall_p1']:.6f}, P99={stats['overall_p99']:.6f}\n")

                f.write("\n")
                f.write("4. è¾“å‡ºæ–‡ä»¶è¯´æ˜\n")
                f.write("-" * 40 + "\n")
                f.write("- stock_descriptive_summary.csv: ä¸»è¦æ±‡æ€»ç»Ÿè®¡è¡¨\n")
                f.write("- stock_percentile_distribution.csv: è¯¦ç»†ç™¾åˆ†ä½æ•°åˆ†å¸ƒ\n")
                f.write("- stock_monthly_cross_sectional_stats.csv: æœˆåº¦æˆªé¢ç»Ÿè®¡æ—¶é—´åºåˆ—\n")
                f.write("- stock_sample_data.csv: åŸå§‹æ•°æ®æ ·æœ¬\n")
                f.write("- analysis_summary_report.txt: æœ¬åˆ†ææŠ¥å‘Š\n")

            print(f"âœ“ åˆ†ææ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

        except Exception as e:
            print(f"ä¿å­˜æ‘˜è¦æŠ¥å‘Šå¤±è´¥: {e}")
            traceback.print_exc()

    def run_analysis(self, start_date='1980-01-01', end_date='2024-12-31',
                     output_dir='/Users/jinanwuyanzu/Desktop/News 5'):
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        print("å¼€å§‹è‚¡ç¥¨æè¿°æ€§ç»Ÿè®¡åˆ†æ...")
        print("=" * 50)
        start_time = datetime.now()

        try:
            if not self.connect_wrds():
                print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥")
                return False
            print("âœ“ æ•°æ®åº“è¿æ¥æˆåŠŸ")

            if not self.load_tickers():
                print("âŒ åŠ è½½tickerå¤±è´¥")
                return False
            print("âœ“ tickeråŠ è½½æˆåŠŸ")

            if not self.get_stock_data(start_date, end_date):
                print("âŒ æ•°æ®è·å–å¤±è´¥")
                return False
            print("âœ“ æ•°æ®è·å–æˆåŠŸ")

            if not self.calculate_indicators():
                print("âŒ æŒ‡æ ‡è®¡ç®—å¤±è´¥")
                return False
            print("âœ“ æŒ‡æ ‡è®¡ç®—æˆåŠŸ")

            if not self.calculate_descriptive_stats():
                print("âŒ ç»Ÿè®¡è®¡ç®—å¤±è´¥")
                return False
            print("âœ“ ç»Ÿè®¡è®¡ç®—æˆåŠŸ")

            if not self.save_results(output_dir):
                print("âŒ ç»“æœä¿å­˜å¤±è´¥")
                return False
            print("âœ“ ç»“æœä¿å­˜æˆåŠŸ")

            # æ˜¾ç¤ºç»“æœ
            print("\n" + "=" * 80)
            print("ğŸ“Š æè¿°æ€§ç»Ÿè®¡ç»“æœæ±‡æ€»")
            print("=" * 80)
            pd.set_option('display.max_columns', None)
            pd.set_option('display.width', None)
            print(self.summary_table.to_string(index=False))

            return True

        except Exception as e:
            print(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°ä¸¥é‡é”™è¯¯: {e}")
            traceback.print_exc()
            return False
        finally:
            if self.db:
                self.db.close()
                print("\nğŸ”Œ æ•°æ®åº“è¿æ¥å·²å…³é—­")
            end_time = datetime.now()
            print(f"\nâ±ï¸ æ€»è€—æ—¶: {end_time - start_time}")
            if hasattr(self, 'summary_table') and self.summary_table is not None:
                print(f"ğŸ‰ åˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
            else:
                print("âš ï¸ åˆ†ææœªæˆåŠŸç”Ÿæˆç»“æœæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ä¿¡æ¯")


# ==================================
#            ä½¿ç”¨ç¤ºä¾‹
# ==================================
if __name__ == "__main__":
    # è¯·å°†è¿™é‡Œçš„è·¯å¾„ä¿®æ”¹ä¸ºæ‚¨è‡ªå·±å­˜æ”¾ ticker_list.csv æ–‡ä»¶çš„å®é™…è·¯å¾„
    ticker_file_path = '../ticker list.csv'

    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = StockDescriptiveStats(ticker_csv_path=ticker_file_path)

    # è¿è¡Œåˆ†æï¼Œç»“æœå°†ä¿å­˜åˆ°æŒ‡å®šç›®å½•
    success = analyzer.run_analysis(
        start_date='1980-01-01',
        end_date='2024-12-31',
        output_dir='../News '
    )

    if success:
        print("\nâœ… æ‰€æœ‰åˆ†ææ­¥éª¤å‡å·²æˆåŠŸå®Œæˆï¼")
        print("ğŸ“ è¾“å‡ºæ–‡ä»¶è¯´æ˜:")
        print("  â€¢ stock_descriptive_summary.csv - ä¸»è¦æ±‡æ€»ç»Ÿè®¡è¡¨")
        print("  â€¢ stock_percentile_distribution.csv - è¯¦ç»†ç™¾åˆ†ä½æ•°åˆ†å¸ƒ")
        print("  â€¢ stock_monthly_cross_sectional_stats.csv - æœˆåº¦æˆªé¢ç»Ÿè®¡æ—¶é—´åºåˆ—")
        print("  â€¢ stock_sample_data.csv - åŸå§‹æ•°æ®æ ·æœ¬")
        print("  â€¢ analysis_summary_report.txt - åˆ†ææ‘˜è¦æŠ¥å‘Š")
    else:
        print("\nâŒ åˆ†æè¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ä¸Šæ–¹é”™è¯¯ä¿¡æ¯")